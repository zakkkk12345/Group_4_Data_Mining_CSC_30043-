import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, confusion_matrix,
    RocCurveDisplay, PrecisionRecallDisplay, ConfusionMatrixDisplay
)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.calibration import CalibratedClassifierCV

import matplotlib.pyplot as plt
import seaborn as sns


# Load data
train = pd.read_csv("Training Data.csv")
test = pd.read_csv("Test data.csv")

TARGET = "Loan Status"
ID_COLS = [c for c in ["Loan ID", "Customer ID"] if c in train.columns]

# Encode target
y = (
    train[TARGET]
    .astype(str)
    .str.lower()
    .str.replace("_", " ")
    .str.strip()
    .str.contains("charged off")
    .astype(int)
)

X = train.drop(columns=[TARGET] + ID_COLS, errors="ignore")
X_test = test.drop(columns=ID_COLS, errors="ignore")


# Missingness bar chart
missing_pct = X.isna().mean().sort_values(ascending=False)
plt.figure(figsize=(10, 4))
missing_pct.plot(kind="bar")
plt.ylabel("Proportion missing")
plt.title("Missingness by feature")
plt.tight_layout()
plt.show()


# Correlation heatmap (numeric + target)
num_cols_all = X.select_dtypes(include=[np.number]).columns
corr_df = X[num_cols_all].copy()
corr_df["TARGET"] = y
corr = corr_df.corr(numeric_only=True)

plt.figure(figsize=(10, 7))
sns.heatmap(corr, cmap="viridis", linewidths=0.2)
plt.title("Correlation heatmap (numeric features + target)")
plt.tight_layout()
plt.show()


# Train/validation split
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)


# Preprocessing
cat_cols = X_train.select_dtypes(include=["object"]).columns.tolist()
num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()

numeric_transformer = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown="ignore"))
])

preprocess = ColumnTransformer([
    ("num", numeric_transformer, num_cols),
    ("cat", categorical_transformer, cat_cols)
])


# Models
base_models = {
    "LogisticRegression": LogisticRegression(max_iter=500, class_weight="balanced"),
    "RandomForest": RandomForestClassifier(
        n_estimators=300, random_state=42, n_jobs=-1, class_weight="balanced"
    ),
    "GradientBoosting": GradientBoostingClassifier(random_state=42)
}

def make_pipeline(estimator):
    return Pipeline([
        ("preprocess", preprocess),
        ("clf", CalibratedClassifierCV(estimator, cv=3, method="isotonic"))
    ])


# Train and evaluate models
results = []
pipes = {}
val_probs = {}

print("Training models...")
for name, model in base_models.items():
    print(f"  Training {name}...")
    pipe = make_pipeline(model)
    pipe.fit(X_train, y_train)
    pipes[name] = pipe

    proba = pipe.predict_proba(X_val)[:, 1]
    val_probs[name] = proba
    pred = (proba >= 0.5).astype(int)

    results.append({
        "Model": name,
        "Accuracy": accuracy_score(y_val, pred),
        "Precision": precision_score(y_val, pred, zero_division=0),
        "Recall": recall_score(y_val, pred, zero_division=0),
        "F1": f1_score(y_val, pred, zero_division=0),
        "ROC_AUC": roc_auc_score(y_val, proba),
        "PR_AUC": average_precision_score(y_val, proba)
    })

results_df = pd.DataFrame(results).sort_values("PR_AUC", ascending=False)
print("\nModel Comparison:")
print(results_df.to_string(index=False))

best_name = results_df.iloc[0]["Model"]
best_proba_val = val_probs[best_name]


# ROC and PR curves
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

RocCurveDisplay.from_predictions(y_val, best_proba_val, ax=axes[0])
axes[0].set_title(f"ROC Curve – {best_name}")

PrecisionRecallDisplay.from_predictions(y_val, best_proba_val, ax=axes[1])
axes[1].set_title(f"PR Curve – {best_name}")

plt.tight_layout()
plt.show()


# F2 threshold tuning (FIXED FORMULA)
def find_best_f2(y_true, proba):
    thresholds = np.linspace(0.01, 0.99, 99)
    best_t, best_f2 = 0.5, -1
    for t in thresholds:
        y_hat = (proba >= t).astype(int)
        p = precision_score(y_true, y_hat, zero_division=0)
        r = recall_score(y_true, y_hat, zero_division=0)
        # Fixed F2 formula: beta=2, so weight recall 2x more than precision
        f2 = 0 if (p + r) == 0 else (5 * p * r) / (4 * p + r)
        if f2 > best_f2:
            best_f2, best_t = f2, t
    return best_t, best_f2


# Calculate F2-optimized threshold and predictions for ALL models
model_thresholds = {}
model_predictions = {}
model_f2_scores = {}

print("\nF2 Threshold Optimization:")
for name in base_models.keys():
    t_f2, f2_score = find_best_f2(y_val, val_probs[name])
    pred_f2 = (val_probs[name] >= t_f2).astype(int)
    
    model_thresholds[name] = t_f2
    model_predictions[name] = pred_f2
    model_f2_scores[name] = f2_score
    
    print(f"  {name}: threshold={t_f2:.3f}, F2={f2_score:.3f}")


# Get best model info
best_name = results_df.iloc[0]["Model"]
best_proba_val = val_probs[best_name]
best_threshold = model_thresholds[best_name]
best_pred = model_predictions[best_name]


# Confusion Matrix for best model
cm = confusion_matrix(y_val, best_pred)
ConfusionMatrixDisplay(cm, display_labels=["Fully Paid", "Charged Off"]).plot()
plt.title(f"Confusion Matrix – {best_name} (F2-optimized)")
plt.show()


# Fairness metric (equal opportunity) for ALL models
def calculate_fairness(y_true, y_pred, income_col):
    """Calculate equal opportunity gap across income quartiles"""
    group = pd.qcut(income_col, q=4, duplicates="drop")
    
    fairness_data = pd.DataFrame({
        'y_true': y_true.values,
        'y_pred': y_pred,
        'group': group.values
    }, index=y_true.index)
    
    # Calculate recall (TPR) for each group
    recall_by_group = fairness_data.groupby('group').apply(
        lambda x: (x['y_pred'] & x['y_true']).sum() / (x['y_true'].sum() + 1e-12)
    )
    eo_gap = recall_by_group.max() - recall_by_group.min()
    
    return eo_gap, recall_by_group


print("\nFAIRNESS ANALYSIS (Equal Opportunity)")

fairness_results = []

for name in base_models.keys():
    eo_gap, recall_by_group = calculate_fairness(
        y_val, 
        model_predictions[name], 
        X_val["Annual Income"]
    )
    
    fairness_results.append({
        "Model": name,
        "Equal_Opportunity_Gap": eo_gap
    })
    
    print(f"\n{name}:")
    print(f"  Equal Opportunity Gap: {eo_gap:.3f}")
    print("  Recall by Income Quartile:")
    for quartile, recall in recall_by_group.items():
        print(f"    {quartile}: {recall:.3f}")

# Visualize fairness comparison
fairness_df = pd.DataFrame(fairness_results)

plt.figure(figsize=(10, 5))
plt.bar(fairness_df["Model"], fairness_df["Equal_Opportunity_Gap"], 
        color=['#1f77b4', '#ff7f0e', '#2ca02c'])
plt.ylabel("Equal Opportunity Gap")
plt.title("Fairness Comparison Across Models\n(Lower is better - indicates more equal treatment across income groups)")
plt.xticks(rotation=45, ha='right')
plt.axhline(y=0.1, color='r', linestyle='--', alpha=0.5, label='0.1 threshold')
plt.legend()
plt.tight_layout()
plt.show()

best_eo_gap = fairness_df[fairness_df["Model"] == best_name]["Equal_Opportunity_Gap"].values[0]


# Final metrics output
print("\nFINAL METRICS (F2-optimized)")
print(f"Model: {best_name}")
print(f"Optimal Threshold: {best_threshold:.3f}")
print(f"Accuracy:  {accuracy_score(y_val, best_pred):.3f}")
print(f"Precision: {precision_score(y_val, best_pred, zero_division=0):.3f}")
print(f"Recall:    {recall_score(y_val, best_pred, zero_division=0):.3f}")
print(f"F2-score:  {model_f2_scores[best_name]:.3f}")
print(f"PR-AUC:    {average_precision_score(y_val, best_proba_val):.3f}")
print(f"Equal Opportunity gap: {best_eo_gap:.3f}")

